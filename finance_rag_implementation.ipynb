{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4573b4f",
   "metadata": {},
   "source": [
    "# Notebook Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ffb36",
   "metadata": {},
   "source": [
    "In this notebook, I implement RAG (Retrieval Augmented Generation) to build a program that allows me to ask a financial question about a company and get a response from ChatGPT that integrates new financial information into that response. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0514dd92",
   "metadata": {},
   "source": [
    "ChatGPT may not have been trained on this new or existing financial information, but I retrieve financial information about the companies from Yahoo Finance summaries and Yahoo Finance news headlines to augment the response of the ChatGPT large language model (LLM). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3f32a5",
   "metadata": {},
   "source": [
    "Even though ChatGPT was not trained on this data, it can incorporate this new data into its reasoning and response. That is the essence of how RAG works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65d8963",
   "metadata": {},
   "source": [
    "Retrieval Augmented Generation, in this context, allows me to connect external knowledge about the financial context of a company to the large language model in a meaningful way. This is done through embedding. Embedding converts the external knowledge into numerical vector representations, where texts with similar meanings are closer together in the vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21899ba3",
   "metadata": {},
   "source": [
    "The first step in the sequence flow of the RAG embedding is to embed your new, external knowledge base. You gather your documents, split them into smaller chunks, and pass each chunk into an embedding model. The embedding model I use here is `all-MiniLM-L6-v2` from HuggingFace. Then I store the resulting vectors from the chunks of information into a vector database, which is `FAISS` here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccc8fb0",
   "metadata": {},
   "source": [
    "The next step in RAG is to embed the query from the user with the same model, here `all-MiniLM-L6-v2`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eab86a",
   "metadata": {},
   "source": [
    "The third step in the RAG sequence is to find similar chunks to the embedded query in the vector database, and retrieve the top `k` (here 3) most relevant chunks based on the information chunks that are closest in the vector space to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70276f8a",
   "metadata": {},
   "source": [
    "Finally, you generate an answer from the LLM by combining the query and the retrieved context chunks, feeding it into the LLM, and then allowing the LLM to use the retrieved text to create more accurate and specific reponses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee705d6",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1d5336",
   "metadata": {},
   "source": [
    "In order to run this code, you will need to set your own secret OpenAI API Key equal to `OPENAI_API_KEY` in bash or your Windows command prompt (CMD). In the Windows CMD, this code is `set OPENAI_API_KEY=your-secret-key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdf046a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mothn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import yfinance as yf\n",
    "import feedparser\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import openai\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c6b31ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set your OpenAI API Key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cabd849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary for company names to tickers\n",
    "company_ticker_map = {\n",
    "    \"microsoft\": \"MSFT\",\n",
    "    \"apple\": \"AAPL\",\n",
    "    \"google\": \"GOOGL\",\n",
    "    \"amazon\": \"AMZN\",\n",
    "    \"meta\": \"META\",\n",
    "    \"tesla\": \"TSLA\",\n",
    "    \"bny\": \"BK\"\n",
    "    # Add more as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efe62b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create utility functions\n",
    "def get_ticker_from_question(question):\n",
    "    question_lower = question.lower()\n",
    "    for name in company_ticker_map:\n",
    "        if re.search(rf\"{name}\", question_lower):\n",
    "            return company_ticker_map[name], name\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6d89eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_summary(ticker):\n",
    "    stock = yf.Ticker(ticker)\n",
    "    info = stock.info\n",
    "    return info.get('longBusinessSummary', 'No summary available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da50f56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yahoo_finance_news(ticker):\n",
    "    url = f\"https://finance.yahoo.com/rss/headline?s={ticker}\"\n",
    "    feed = feedparser.parse(url)\n",
    "    news_texts = [entry['title'] + \". \" + entry['summary'] for entry in feed['entries']]\n",
    "    return news_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85f5ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=300):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "723eb1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_corpus(ticker, company_name):\n",
    "    summary = get_company_summary(ticker)\n",
    "    summary_chunks = chunk_text(summary)\n",
    "\n",
    "    news_articles = get_yahoo_finance_news(ticker)\n",
    "    news_chunks = [chunk_text(article) for article in news_articles]\n",
    "    news_chunks = [item for sublist in news_chunks for item in sublist]\n",
    "\n",
    "    return summary_chunks + news_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a79b59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full RAG pipeline\n",
    "def run_rag_with_news(question):\n",
    "    ticker, company_name = get_ticker_from_question(question)\n",
    "    if not ticker:\n",
    "        return \"Sorry, I couldn't identify a company in your question.\"\n",
    "\n",
    "    corpus = build_rag_corpus(ticker, company_name)\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(corpus)\n",
    "\n",
    "    index = faiss.IndexFlatL2(embeddings[0].shape[0])\n",
    "    index.add(np.array(embeddings))\n",
    "\n",
    "    def retrieve_context(q):\n",
    "        q_embed = model.encode([q])\n",
    "        D, I = index.search(np.array(q_embed), 3)\n",
    "        return \"\\n\".join([corpus[i] for i in I[0]])\n",
    "\n",
    "    context = retrieve_context(question)\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {question}\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d77b29d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mothn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Answer:\n",
      " BNY Mellon has signed a multiyear deal with OpenAI, gaining access to cutting-edge AI tools such as Deep Research and advanced reasoning models. Additionally, BNY has made a minority investment in EquiLend, a company in the securities finance industry.\n"
     ]
    }
   ],
   "source": [
    "#example search\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"What is BNY doing in AI this year?\"\n",
    "    answer = run_rag_with_news(question)\n",
    "    print(\"\\nGenerated Answer:\\n\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
